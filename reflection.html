<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Reflections</title>
    <link rel="stylesheet" href="style4.css"> <!-- Link to your existing CSS file -->
</head>
<body>
    <!-- Navigation Bar (Same as homepage) -->
    <nav id="navbar" class="navbar">
        <div class="logo">Prasanna's Portfolio</div>
        <ul class="nav-links">
            <li><a href="index.html">Home</a></li>
            <li><a href="introduction.html">Introduction</a></li>
            <li><a href="project.html">Projects</a></li>
            <li><a href="reflection.html" class="active">Reflections</a></li>
        </ul>
    </nav>

    <!-- Main Content Area -->
    <div class="container">
        <h1>Reflections on Data Structures and Algorithms</h1>
  <p style="text-align: center; font-size: 1.2rem; color: #555; margin-bottom: 20px;">
        Click on a question to expand and see the answer.
    </p>
        <!-- Accordion for Reflection Questions -->
        <div class="accordion">
            <div class="accordion-item">
                <input type="radio" name="accordion" id="item1" class="accordion-input" checked>
                <label for="item1" class="accordion-header">1. What are the kinds of problems we see in nature?</label>
                <div class="accordion-content">
    The kinds of problems we see in nature often involve iteration, recursion, and backtracking, each used to solve specific types of challenges:
    <ul>
        <li><strong>Iteration:</strong> Iteration involves repeatedly applying a set of operations until a condition is met. This is commonly seen in problems where you need to process every element in a collection, such as calculating the sum of elements in an array or iterating through a range of values. For example, iterating through a list to find the maximum value or performing a task for a number of iterations is a typical problem handled by iteration.</li>
        <li><strong>Recursion:</strong> Recursion involves solving a problem by breaking it down into smaller subproblems that resemble the original problem. It's especially useful when a problem can be divided into smaller, self-similar problems. Classic examples include computing the Fibonacci sequence, solving the Tower of Hanoi problem, or implementing a recursive search in a tree structure. Recursion is ideal for problems where the solution involves repeated function calls that eventually lead to the base case.</li>
        <li><strong>Backtracking:</strong> Backtracking is a method of exploring all possible solutions incrementally and systematically, undoing decisions when a solution path fails. This technique is commonly used in problems that require searching through many possibilities, such as in puzzles like the N-Queens problem or solving mazes. Backtracking helps eliminate non-promising paths early, improving efficiency in finding the correct solution.</li>
    </ul>
</div>




            </div>

            <div class="accordion-item">
                <input type="radio" name="accordion" id="item2" class="accordion-input">
                <label for="item2" class="accordion-header">2. What is space and time efficiency? Why are they important? Explain the different class of problems and orders of growth.</label>
                <div class="accordion-content">
    <strong>Space Efficiency:</strong> Refers to the amount of memory an algorithm uses during its execution. A space-efficient algorithm minimizes memory usage. For example, in-place sorting algorithms like Bubble Sort are space-efficient because they don’t require extra memory.
    <br><strong>Time Efficiency:</strong> Refers to the time an algorithm takes to complete. More time-efficient algorithms minimize the number of operations, reducing execution time. For example, Merge Sort and Quick Sort are more time-efficient than Bubble Sort for large datasets.

    <h3>Why are They Important?</h3>
    <ul>
        <strong>Space Efficiency:</strong> Limited resources like memory may lead to overflows or inefficient performance if an algorithm uses excessive memory. <br>
    <strong>Time Efficiency:</strong> The speed of an algorithm affects its practicality, especially for large datasets. Algorithms with poor time efficiency can cause delays or failures in real-time applications.
    </ul>

    <h3>Classes of Problems and Orders of Growth</h3>
    <ul>
        <li><strong>O(1) – Constant time:</strong> Independent of input size (e.g., accessing an array element).</li>
        <li><strong>O(log n) – Logarithmic time:</strong> Grows slowly (e.g., Binary Search).</li>
        <li><strong>O(n) – Linear time:</strong> Grows with input size (e.g., Linear Search).</li>
        <li><strong>O(n log n) – Linearithmic time:</strong> More efficient sorting (e.g., Merge Sort, Quick Sort).</li>
        <li><strong>O(n²) – Quadratic time:</strong> Grows with nested iterations (e.g., Bubble Sort).</li>
        <li><strong>O(2^n) – Exponential time:</strong> Grows rapidly, often impractical for large inputs (e.g., brute-force solutions).</li>
        <li><strong>O(n!) – Factorial time:</strong> Extremely slow for large inputs (e.g., brute-force N-Queens Problem).</li>
    </ul>
</div>

            </div>

            <div class="accordion-item">
                <input type="radio" name="accordion" id="item3" class="accordion-input">
                <label for="item3" class="accordion-header">3. Takeaway from different design principles (Chapter 2)</label>
                <div class="accordion-content">
    <ul>
        <strong>The Takeaways from Design Principles are:</strong>
        <li><strong>Brute Force:</strong> Brute force solves problems by exhaustively trying all possible solutions. It's a straightforward approach but can be very inefficient, especially for large inputs. For example, linear search checks every element in the array until it finds the target or finishes checking all elements, making it O(n) in time complexity.</li>
        <li><strong>Decrease and Conquer:</strong> This technique involves breaking down a problem into smaller instances of the same problem and solving them in a systematic manner. Binary search is a classic example where the problem size is reduced by half in each step, improving search time compared to brute force (O(log n) vs O(n)).</li>
        <li><strong>Divide and Conquer:</strong> Divide and conquer splits the problem into independent subproblems, solves them separately, and then combines their results. Merge Sort is a prime example, which divides an array into two halves, recursively sorts each half, and merges them back together, giving an efficient time complexity of O(n log n).</li>
        <li><strong>Transform and Conquer:</strong> This approach transforms the problem into a more manageable form. For instance, Heap Sort builds a heap structure from the input array and sorts it by repeatedly extracting the maximum element, which optimizes the sorting process compared to simpler algorithms like selection sort.</li>
        <li><strong>Dynamic Programming:</strong> Dynamic programming solves problems by breaking them into smaller overlapping subproblems and storing their solutions to avoid recalculating the same thing multiple times. A classic example is the Fibonacci sequence, where using a simple recursive approach would be slow, but memoization stores previously computed values, improving efficiency.</li>
        <li><strong>Greedy Technique:</strong> Greedy algorithms work by making the best possible decision at each step, aiming for an overall optimal solution. Dijkstra’s Algorithm is a good example where, at each step, the shortest unvisited node is chosen, eventually resulting in the shortest path from the source node in a graph.</li>
        <li><strong>Space and Time Trade-off:</strong> This technique aims to optimize both time and space. Memoization is a prime example, where intermediate results are stored in a cache (using extra space) to avoid redundant calculations, which ultimately saves time in recursive algorithms like those for the Fibonacci sequence.</li>
        <li><strong>Randomized Algorithms:</strong> These algorithms rely on randomization to improve performance and handle certain problems more efficiently. Quick Sort is a typical example, where a random pivot element is chosen in each iteration, making the algorithm faster on average and reducing the worst-case performance.</li>
        <li><strong>Backtracking:</strong> Backtracking is a trial-and-error method that solves problems incrementally, undoing decisions when a solution path does not work out. It’s widely used in constraint satisfaction problems like the N-Queens problem, where you place queens on a chessboard and backtrack if a queen is placed in an invalid position.</li>
    </ul>
</div>

            </div>

            <div class="accordion-item">
                <input type="radio" name="accordion" id="item4" class="accordion-input">
                <label for="item4" class="accordion-header">4. The hierarchical data and how different tree data structures solve and optimize over problem scenarios</label>
                <div class="accordion-content">
    Hierarchical data represents a structure where elements are organized in a tree-like format, with a single root node and multiple levels of child nodes beneath it. These data structures are useful for modeling relationships like family trees, organizational charts, and file systems. Various tree data structures optimize specific scenarios by offering different strengths in terms of efficiency, flexibility, and balance.
    <ul>
        <li><strong>Tree:</strong> A general tree structure allows nodes to have multiple children. It is used in scenarios where elements naturally have more than two child nodes. It is flexible but doesn't enforce strict ordering or balancing, which can lead to inefficiencies in certain operations like searching or inserting.</li>
        <li><strong>Binary Search Tree (BST):</strong> A binary tree where each node has at most two children and the left child is less than the parent, while the right child is greater. BSTs are ideal for searching, insertion, and deletion operations but can become unbalanced, leading to inefficient operations in the worst case (O(n) time complexity).</li>
        <li><strong>AVL Tree:</strong> An AVL (Adelson-Velsky and Landis) tree is a self-balancing binary search tree where the difference in heights of left and right subtrees is no more than one. This balancing ensures that operations like search, insertion, and deletion remain efficient (O(log n) time complexity), even in the worst case, solving the problem of unbalanced BSTs.</li>
        <li><strong>2-3 Tree:</strong> A 2-3 tree is a balanced search tree in which every node can contain either two or three children. It guarantees that all leaves are at the same level, ensuring balanced operations. 2-3 trees are useful for scenarios requiring balanced search and efficient insertions, such as databases or file indexing systems.</li>
        <li><strong>Red-Black Tree:</strong> A red-black tree is a self-balancing binary search tree with an additional set of properties that ensures the tree remains balanced. The balance of the tree is maintained by enforcing color properties and restructuring operations during insertions and deletions. Red-Black trees provide efficient operations (O(log n) time complexity) and are widely used in databases and memory management systems.</li>
        <li><strong>Heap:</strong> A heap is a complete binary tree used for efficiently implementing priority queues. In a min-heap, the parent node is always smaller than its children, and in a max-heap, the parent node is always larger. Heaps optimize operations like finding the minimum or maximum element in O(1) time and performing insertion or deletion in O(log n) time. They are often used in algorithms like Dijkstra’s shortest path and heap sort.</li>
        <li><strong>Trie:</strong> A trie is a tree-like data structure used for storing a set of strings, where each node represents a character in a string. Tries are particularly useful for searching and autocomplete operations, as they allow for efficient prefix-based searches in O(m) time, where m is the length of the string. Tries optimize search times in scenarios involving large sets of strings, like dictionary lookups or spell checkers.</li>
    </ul>
</div>

            </div>

            <div class="accordion-item">
                <input type="radio" name="accordion" id="item5" class="accordion-input">
                <label for="item5" class="accordion-header">5. The need for array query algorithms and their implications</label>
                <div class="accordion-content">
    Array query algorithms are designed to optimize operations on arrays, which are widely used data structures in computer science. Arrays allow efficient access to elements by index, but they can face challenges when it comes to complex queries, such as range queries, prefix sums, or frequent updates. Array query algorithms help in solving these problems efficiently, making operations faster and more suitable for real-time applications.
    <ul>
        <li><strong>The need for efficient array query algorithms arises primarily due to the following reasons:</strong></li>
        <ul>
            <li><strong>Handling Large Data Sets:</strong> When arrays grow in size, operations like searching, inserting, or modifying elements can become time-consuming. Array query algorithms help by providing a systematic approach to reduce the time complexity of operations, especially when large arrays are involved.</li>
            <li><strong>Frequent Updates and Queries:</strong> In scenarios where arrays are frequently updated, such as dynamic data processing, we need algorithms that can handle updates and queries in a time-efficient manner. For example, in real-time data analytics, maintaining fast access to the modified data is crucial.</li>
            <li><strong>Range Queries:</strong> When dealing with tasks like querying the sum of elements in a subarray or finding the minimum/maximum element in a subarray, direct approaches can be inefficient. Array query algorithms, such as Segment Trees, Fenwick Trees (Binary Indexed Trees), and Sparse Tables, are designed to address such problems efficiently, providing fast query results even after multiple updates.</li>
            <li><strong>Prefix or Suffix Queries:</strong> In scenarios where cumulative sums or other aggregations are required, array query algorithms like Prefix Sum Arrays optimize the computation, providing quick access to cumulative information without having to scan the entire array each time.</li>
        </ul>
        <li><strong>Applications of Array Query Algorithms</strong></li>
        <ul>
            <li><strong>Range Query Problems:</strong> These problems involve querying a specific property (such as sum, minimum, maximum) over a range of elements in an array. Common examples are:
                <ul>
                    <li>Sum Queries: Querying the sum of elements between two indices in an array.</li>
                    <li>Range Minimum Query (RMQ): Finding the minimum value within a specific range of the array.</li>
                </ul>
                Algorithms like Segment Trees, Fenwick Trees, and Sparse Tables are commonly used to solve such problems with optimal time complexities.</li>
            <li><strong>Dynamic Data Queries:</strong> In applications where data changes frequently, such as stock prices, sensor readings, or user actions, we often need to perform dynamic queries efficiently. Using algorithms like Binary Indexed Trees (Fenwick Tree), we can efficiently update an element and query aggregate information in logarithmic time.</li>
            <li><strong>Prefix Sum Calculation:</strong> One of the simplest yet powerful array queries is the calculation of prefix sums. By precomputing the prefix sum array, we can answer range sum queries in constant time.
                <ul>
                    <li>Example: In a financial application, calculating the total sales revenue over different time periods.</li>
                </ul>
            </li>
            <li><strong>Searching and Sorting:</strong> Though not technically classified as "queries," efficient searching and sorting of arrays are vital operations that impact query performance. Algorithms like Binary Search optimize searching, and Merge Sort and Quick Sort are efficient sorting algorithms that can help in preparing data for querying.</li>
        </ul>
        <li><strong>Principles Behind Array Query Algorithms</strong></li>
        <ul>
            <li><strong>Preprocessing:</strong> Many efficient array query algorithms rely on preprocessing the array to build auxiliary data structures that allow faster querying. This preprocessing typically involves constructing data structures that enable efficient lookups, like Segment Trees, Fenwick Trees, or Sparse Tables. These structures allow us to perform multiple queries on the array in logarithmic or constant time after the initial setup.</li>
            <li><strong>Divide and Conquer:</strong> Some array query algorithms leverage the divide-and-conquer paradigm. For example, Segment Trees divide the array into smaller segments, allowing them to recursively break down range queries into smaller subqueries that can be resolved efficiently.</li>
            <li><strong>Lazy Propagation:</strong> In certain types of array queries, such as range updates and queries, algorithms like Segment Trees use lazy propagation to delay updates until necessary, which helps in minimizing unnecessary recomputation.</li>
            <li><strong>Dynamic Updates:</strong> In scenarios where the array undergoes frequent updates (insertions, deletions, or modifications), algorithms like Binary Indexed Trees (Fenwick Tree) and Segment Trees allow efficient updates in O(log n) time, preventing the need for linear-time updates.</li>
            <li><strong>Efficient Querying:</strong> The goal of array query algorithms is to optimize the querying process by reducing the time complexity. Algorithms like Fenwick Trees and Segment Trees allow range queries (sum, min, max) to be answered in logarithmic time, significantly improving performance for large arrays.</li>
        </ul>
    </ul>
</div>

            </div>

            <div class="accordion-item">
                <input type="radio" name="accordion" id="item6" class="accordion-input">
                <label for="item6" class="accordion-header">6. Differentiate between trees and graphs and their traversals. Applications of each.</label>
                <div class="accordion-content">
        
        <h3>Difference Between Trees and Graphs</h3>
        <!-- Table for Difference -->
        <table>
            <thead>
                <tr>
                    <th>Feature</th>
                    <th>Tree</th>
                    <th>Graph</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Definition</strong></td>
                    <td>A tree is a connected, acyclic graph with one root and a hierarchical structure.</td>
                    <td>A graph is a collection of nodes connected by edges. It can be cyclic or acyclic.</td>
                </tr>
                <tr>
                    <td><strong>Edges</strong></td>
                    <td>A tree has exactly n-1 edges for n nodes.</td>
                    <td>A graph can have any number of edges and can have cycles.</td>
                </tr>
                <tr>
                    <td><strong>Connectivity</strong></td>
                    <td>Always connected.</td>
                    <td>Can be connected or disconnected.</td>
                </tr>
                <tr>
                    <td><strong>Cycle</strong></td>
                    <td>No cycles allowed.</td>
                    <td>Can contain cycles.</td>
                </tr>
                <tr>
                    <td><strong>Path</strong></td>
                    <td>Exactly one path between any two nodes.</td>
                    <td>Multiple paths can exist between two nodes.</td>
                </tr>
                <tr>
                    <td><strong>Root</strong></td>
                    <td>Has a root node.</td>
                    <td>No root, can have any node as a starting point.</td>
                </tr>
                <tr>
                    <td><strong>Hierarchical Structure</strong></td>
                    <td>Strict hierarchy with parent-child relationship.</td>
                    <td>No strict hierarchy, can represent complex relationships.</td>
                </tr>
            </tbody>
        </table>

        <h3>Traversal Methods for Trees and Graphs</h3>

        <!-- Tree Traversal Methods -->
        <h4>Tree Traversal Methods</h4>
        <table>
            <thead>
                <tr>
                    <th>Traversal Type</th>
                    <th>Description</th>
                    <th>Application</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>In-order Traversal</td>
                    <td>Visit left subtree, root, and then right subtree.</td>
                    <td>Used in binary search trees (BST) to retrieve nodes in sorted order.</td>
                </tr>
                <tr>
                    <td>Pre-order Traversal</td>
                    <td>Visit root, then left and right subtrees.</td>
                    <td>Used in expression tree evaluation or when cloning a tree.</td>
                </tr>
                <tr>
                    <td>Post-order Traversal</td>
                    <td>Visit left, right subtrees, and then root.</td>
                    <td>Used for deleting a tree, calculating postfix expressions.</td>
                </tr>
                <tr>
                    <td>Level-order Traversal</td>
                    <td>Visit nodes level by level, using a queue.</td>
                    <td>Typically used to print nodes level by level or breadth-first search (BFS).</td>
                </tr>
            </tbody>
        </table>

        <!-- Graph Traversal Methods -->
        <h4>Graph Traversal Methods</h4>
        <table>
            <thead>
                <tr>
                    <th>Traversal Type</th>
                    <th>Description</th>
                    <th>Application</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Depth-First Search (DFS)</td>
                    <td>Explore as deep as possible along each branch before backtracking.</td>
                    <td>Used in cycle detection, connected components, and topological sorting.</td>
                </tr>
                <tr>
                    <td>Breadth-First Search (BFS)</td>
                    <td>Explore nodes level by level using a queue.</td>
                    <td>Used in shortest path algorithms (unweighted graphs), network flow, and web crawling.</td>
                </tr>
            </tbody>
        </table>

        <h3>Applications of Trees</h3>
        <ul>
            <li>Binary Search Tree (BST): Efficient search, insertion, and deletion of data. Used in databases, search engines.</li>
            <li>Heap: Implements priority queues for min-heap or max-heap. Used in job scheduling and Dijkstra’s algorithm.</li>
            <li>Trie (Prefix Tree): Efficient string matching and prefix-based searching. Used in autocomplete systems and dictionary implementations.</li>
            <li>Expression Tree: Represents and evaluates mathematical expressions. Used in compiler optimization and expression evaluation.</li>
            <li>Decision Tree: Used for classification and regression in machine learning. Applied in predictive analytics and classification tasks.</li>
        </ul>

        <h3>Applications of Graphs</h3>
        <ul>
            <li>Social Network Graph: Represents relationships between users. Applied in social media and friend recommendation systems.</li>
            <li>Shortest Path Algorithms: Used to find the shortest path in weighted graphs. Applied in GPS navigation systems and network routing.</li>
            <li>Network Flow Problems: Solves max flow problems in network design. Applied in data packet routing and telecommunications.</li>
            <li>Web Crawling: Traverses and analyzes linked web pages. Used in search engines and web page ranking (e.g., PageRank).</li>
            <li>Cycle Detection: Detects cycles in graphs. Applied in deadlock detection in operating systems and dependency graphs in build systems.</li>
        </ul>

    </div>

            </div>

            <div class="accordion-item">
                <input type="radio" name="accordion" id="item7" class="accordion-input">
                <label for="item7" class="accordion-header">7. Sorting and searching algorithms and their connection to the real world</label>
                <div class="accordion-content">
    <li><strong>Sorting Algorithms:</strong> Sorting arranges data in a specific order to improve readability and processing. Examples include:</li>
    <ul>
        <li><strong>Bubble Sort:</strong> A simple algorithm ideal for small datasets, often used in teaching.</li>
        <li><strong>Merge Sort:</strong> A stable and efficient divide-and-conquer algorithm, widely used in databases and external sorting where stability matters.</li>
        <li><strong>Quick Sort:</strong> Known for its speed and efficiency in large-scale sorting, it’s a backbone for many system-level sorting tasks like search engine optimization.</li>
        <li><strong>Heap Sort:</strong> Memory-efficient, often used in real-time systems requiring constant sorting operations.</li>
    </ul>

    <li><strong>Searching Algorithms:</strong> Searching involves finding elements in datasets with precision and speed. Examples include:</li>
    <ul>
        <li><strong>Linear Search:</strong> Suitable for unsorted or small data collections.</li>
        <li><strong>Binary Search:</strong> Works on sorted datasets, commonly applied in library systems and directory lookups.</li>
        <li><strong>Hashing:</strong> Offers constant-time lookups, making it invaluable for password management, caching, and data storage systems.</li>
    </ul>

    <li><strong>Real-world Applications:</strong> Sorting and searching are integral in e-commerce platforms for product listing and search, navigation systems for determining shortest routes, database indexing for efficient data retrieval, data preprocessing in analytics, and cybersecurity for secure access and authentication.</li>
</div>

            </div>

            <div class="accordion-item">
                <input type="radio" name="accordion" id="item8" class="accordion-input">
                <label for="item8" class="accordion-header">8. Importance of graph algorithms with respect to spanning trees and shortest paths</label>
                <div class="accordion-content">
    <li><strong>Graph Algorithms:</strong> Graph algorithms are crucial in solving problems involving networks, where entities are represented as nodes and connections as edges. Two key applications are <strong>spanning trees</strong> and <strong>shortest paths</strong>, which have significant real-world importance:</li>

    <li><strong>Spanning Trees:</strong></li>
    <ul>
        <li>A spanning tree is a subset of a graph that includes all the nodes with the minimum number of edges, forming a tree structure without cycles.</li>
        <li><strong>Minimum Spanning Tree (MST):</strong> Algorithms like <strong>Prim's</strong> and <strong>Kruskal's</strong> are used to find the most efficient way to connect all nodes at minimal cost.</li>
        <li><strong>Applications:</strong> Network design (e.g., laying cables for internet or electricity), clustering in data analysis, and designing transportation or communication systems.</li>
    </ul>

    <li><strong>Shortest Path Algorithms:</strong></li>
    <ul>
        <li>Identify the minimum distance or cost between nodes in a graph.</li>
        <li><strong>Dijkstra's Algorithm:</strong> Suitable for graphs with non-negative edge weights, commonly used in navigation systems and network routing.</li>
        <li><strong>Bellman-Ford Algorithm:</strong> Handles graphs with negative weights and is used in scenarios like financial arbitrage and network delay optimization.</li>
        <li><strong>Floyd-Warshall Algorithm:</strong> Computes shortest paths between all pairs of nodes, ideal for dense graphs in applications like logistics and traffic management.</li>
    </ul>

    <li><strong>Importance:</strong> These algorithms optimize resource allocation, improve efficiency in logistics and transportation, facilitate network design, and enable effective decision-making in numerous domains such as telecommunications, urban planning, and computer science. Their ability to model and solve complex problems makes them indispensable in both theoretical and practical applications.</li>
</div>

            </div>

            <div class="accordion-item">
                <input type="radio" name="accordion" id="item9" class="accordion-input">
                <label for="item9" class="accordion-header">9. Discuss different studied algorithm design techniques</label>
                <div class="accordion-content">
    <li><strong>Algorithm Design Techniques:</strong> The different studied algorithm design techniques are as follows:</li>
    
    <h3>1. Brute Force</h3>
    <ul>
        <li>Solves problems by exhaustively trying all possible solutions. It is a straightforward but inefficient approach for large inputs.</li>
        <li><strong>Example:</strong> Linear Search, which checks every element until it finds the target or exhausts the list.</li>
        <li><strong>Time Complexity:</strong> O(n).</li>
    </ul>

    <h3>2. Decrease and Conquer</h3>
    <ul>
        <li>This technique involves breaking a problem into smaller instances of the same problem and solving them systematically.</li>
        <li><strong>Example:</strong> Binary Search, which reduces the problem size by half in each step, improving search efficiency compared to brute force.</li>
        <li><strong>Time Complexity:</strong> O(log n).</li>
    </ul>

    <h3>3. Divide and Conquer</h3>
    <ul>
        <li>The problem is split into independent subproblems, solved separately, and their results are combined.</li>
        <li><strong>Example:</strong> Merge Sort, which divides an array into halves, recursively sorts each half, and merges them.</li>
        <li><strong>Time Complexity:</strong> O(n log n).</li>
    </ul>

    <h3>4. Transform and Conquer</h3>
    <ul>
        <li>The problem is transformed into a more manageable form, often optimizing it for better efficiency.</li>
        <li><strong>Example:</strong> Heap Sort, which builds a heap structure and sorts by repeatedly extracting the maximum element.</li>
        <li><strong>Time Complexity:</strong> O(n log n).</li>
    </ul>

    <h3>5. Dynamic Programming</h3>
    <ul>
        <li>This approach breaks problems into overlapping subproblems and stores solutions to avoid redundant work.</li>
        <li><strong>Example:</strong> Fibonacci Sequence, where memoization stores previously computed values to speed up calculations.</li>
        <li><strong>Time Complexity:</strong> O(n).</li>
    </ul>

    <h3>6. Greedy Technique</h3>
    <ul>
        <li>Greedy algorithms make the best decision at each step, aiming for an overall optimal solution.</li>
        <li><strong>Example:</strong> Dijkstra's Algorithm, which selects the shortest unvisited node at each step to find the shortest path.</li>
        <li><strong>Time Complexity:</strong> O(V log V) for graph-based problems.</li>
    </ul>

    <h3>7. Space and Time Trade-off</h3>
    <ul>
        <li>This technique optimizes both time and space. Extra space is used to store intermediate results, reducing redundant calculations.</li>
        <li><strong>Example:</strong> Memoization in the Fibonacci sequence, where previously computed results are cached to save time.</li>
        <li><strong>Time Complexity:</strong> Varies based on space usage.</li>
    </ul>

    <h3>8. Randomized Algorithms</h3>
    <ul>
        <li>These algorithms use randomization to improve performance and handle specific problems efficiently.</li>
        <li><strong>Example:</strong> Quick Sort, where a random pivot is chosen to reduce worst-case performance.</li>
        <li><strong>Time Complexity:</strong> O(n log n) on average, but can degrade to O(n²) in the worst case.</li>
    </ul>

    <h3>9. Backtracking</h3>
    <ul>
        <li>A trial-and-error method that solves problems incrementally, undoing decisions when a solution path fails.</li>
        <li><strong>Example:</strong> N-Queens problem, where queens are placed incrementally, and the algorithm backtracks if a solution is invalid.</li>
        <li><strong>Time Complexity:</strong> Exponential in nature, depending on the problem size.</li>
    </ul>
</div>

            </div>
        </div>
    </div>
</body>
</html>
